[Strona gÅ‚Ã³wna](../README.md)
# Autoenkodery
Autoenkoder jest rodzajem sieci neuronowej, ktÃ³rej zadaniem jest wytworzenie reprezentacji danych wejÅ›ciowych.
ArchitekturÄ… przypominajÄ… standardowÄ… sieÄ‡ neuronowÄ… ze sprzÄ™Å¼eniem zwrotnym (z tego wzglÄ™du moÅ¼emy stosowaÄ‡ takie same techniki uczenia siÄ™ jak np. propagacja wsteczna), ale ich cel rÃ³Å¼ni siÄ™.
PatrzÄ…c gÅ‚Ä™biej, przeksztaÅ‚ca on dane wejÅ›ciowe w reprezentacjÄ™ przejÅ›ciowÄ…, a nastÄ™pnie na podstawie tej reprezentacji przejÅ›ciowej prÃ³buje odtworzyÄ‡ wynik, ktÃ³ry przypomina dane wejÅ›ciowe.
Ze wzglÄ™du na to, Å¼e w procesie uczenia nie potrzebuje etykiet, to proces ten moÅ¼e przebiegaÄ‡ w sposÃ³b nienadzorowany.

![Architektura podstawowego autoenkodera [1]](../resources/autoencoder-model.png)  
*Architektura podstawowego autoenkodera [1]*

Podstawowy autoenkoder skÅ‚ada siÄ™ z trzech zasadniczych czÄ™Å›ci:
* Koder (ang. encoder) â€“ jego zadaniem jest zakodowanie informacji wejÅ›ciowych do postaci reprezentacji o zmniejszonym wymiarze.
* Zakodowany wektor â€“ jest to warstwa ukryta w samym Å›rodku architektury, nazywana rÃ³wnieÅ¼ kodem. Jest wynikiem kodowania.
* Dekoder (ang. decoder) â€“ ma za zadanie przeksztaÅ‚ciÄ‡ zakodowany wektor do postaci, ktÃ³ra jest jak najbliÅ¼sza wektorowi wejÅ›ciowemu sieci.

```
ğ‘¥ âˆ’ ğ‘¤ğ‘’ğ‘˜ğ‘¡ğ‘œğ‘Ÿ ğ‘¤ğ‘’ğ‘—Å›ğ‘ğ‘–ğ‘œğ‘¤ğ‘¦
ğ‘“(ğ‘¥) âˆ’ ğ‘“ğ‘¢ğ‘›ğ‘˜ğ‘ğ‘—ğ‘ ğ‘˜ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘
h = ğ‘“(ğ‘¥) âˆ’ ğ‘§ğ‘ğ‘˜ğ‘œğ‘‘ğ‘œğ‘¤ğ‘ğ‘›ğ‘¦ ğ‘¤ğ‘’ğ‘˜ğ‘¡ğ‘œğ‘Ÿ
ğ‘”(h) = ğ‘”(ğ‘“(ğ‘¥)) âˆ’ ğ‘“ğ‘¢ğ‘›ğ‘˜ğ‘ğ‘—ğ‘ ğ‘‘ğ‘’ğ‘˜ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘
ğ‘¦ = ğ‘”(h) âˆ’ ğ‘¤ğ‘’ğ‘˜ğ‘¡ğ‘œğ‘Ÿ ğ‘¤ğ‘¦ğ‘—Å›ğ‘ğ‘–ğ‘œğ‘¤ğ‘¦ (ğ‘§ğ‘¤ğ‘ğ‘›ğ‘¦ ğ‘Ÿğ‘’ğ‘˜ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘˜ğ‘ğ‘—Ä…)
ğ‘¥â‰ˆğ‘¦
```

## Zastosowania autoenkoderÃ³w
Architektura autoenkoderÃ³w wydaje siÄ™ byÄ‡ prosta, ale ich zastosowania wbrew pozorom sÄ… bardzo szerokie [1]:
* Redukcja wymiarowoÅ›ci â€“ podstawowe zastosowanie autoenkoderÃ³w. JeÅ›li warstwa z zakodowanym wektorem posiada mniej wymiarÃ³w niÅ¼ warstwa wejÅ›ciowa, to otrzymujemy reprezentacjÄ™ danych wejÅ›ciowych ze zredukowanÄ… liczbÄ… wymiarÃ³w. W takim przypadku moÅ¼emy mÃ³wiÄ‡ o kompresji danych.
* Generowanie nowych wymiarÃ³w â€“ jeÅ›li wektor reprezentacji bÄ™dzie miaÅ‚ wiÄ™cej wymiarÃ³w niÅ¼ wektor wejÅ›ciowy moÅ¼emy uzyskaÄ‡ nowe cechy reprezentujÄ…ce dane wejÅ›ciowe.
* Wykrywanie anomalii â€“ anomalie wystÄ™pujÄ… rzadko w zbiorze danych, wiÄ™c autoenkoder powinien mieÄ‡ problemy z rekonstrukcjÄ… prÃ³bki, ktÃ³ra wykracza poza rozkÅ‚ad danych trenujÄ…cych. PorÃ³wnujÄ…c wartoÅ›ci funkcji straty autoenkodera moÅ¼emy Å‚atwo wykryÄ‡ prÃ³bkÄ™, ktÃ³rej nie udaÅ‚o siÄ™ dostatecznie dobrze zrekonstruowaÄ‡.
* Generowanie danych przypominajÄ…cych dane wejÅ›ciowe â€“ moÅ¼emy to osiÄ…gnÄ…Ä‡ dodajÄ…c losowe szumy do Å›rodkowej warstwy autoenkodera czyli modyfikujÄ…c zakodowany wektor.
* Redukcja szumÃ³w â€“ wykorzystujÄ…c autoenkodery odszumiajÄ…ce (ktÃ³rych architektura jest opisana poniÅ¼ej) jesteÅ›my wstanie redukowaÄ‡ szumy i rÃ³Å¼nego rodzaju zakÅ‚Ã³cenia danych wejÅ›ciowych. MoÅ¼e to zostaÄ‡ wykorzystane jako jedna z metod obrony przed antagonistycznymi przykÅ‚adami.

## Rodzaje autoenkoderÃ³w

### GÅ‚Ä™bokie
SkÅ‚adajÄ… siÄ™ z wielu ukrytych warstw (w czÄ™Å›ci kodera i dekodera) w celu uzyskania bardziej zÅ‚oÅ¼onych kodowaÅ„ czy wyÅ‚apywania skomplikowanych zaleÅ¼noÅ›ci z danych wejÅ›ciowych.

### Rzadkie (ang. sparse autoencoders)
Tego rodzaju autoenkodery nakÅ‚adajÄ… ograniczenia na funkcje aktywacji w ukrytych warstwach sieci. W ten sposÃ³b unikane jest nadmierne dopasowanie, a takÅ¼e poprawiana jest odpornoÅ›Ä‡ na ataki antagonistyczne. PozwalajÄ… one na aktywacjÄ™ jedynie niewielkiej liczby neuronÃ³w w tym samym czasie (przykÅ‚adowo 5%). DziÄ™ki takiemu podejÅ›ciu poziom rekonstrukcji danych obniÅ¼a siÄ™, ale wyÅ‚apywane sÄ… jedynie najbardziej znaczÄ…ce cechy.

### OdszumiajÄ…ce (ang. denoising autoencoders)
Architektura samego autoenkodera odszumiajÄ…cego jest identyczna jak podstawowego autoenkodera, z tÄ… rÃ³Å¼nicÄ… jednak, Å¼e na wejÅ›ciu do sieci neuronowej wrzucane sÄ… zaszumione dane, a funkcja straty liczona jest wzglÄ™dem oryginalnego wejÅ›cia, bez dodatkowego szumu.

![Architektura autoenkodera odszumiajÄ…cego [1]](../resources/denoising-autoencoder.png)  
*Architektura autoenkodera odszumiajÄ…cego [1]*

### Kurczliwe (ang. contractive autoencoders)
W tym przypadku funkcja straty jest skonstruowana w taki sposÃ³b, Å¼eby pozbyÄ‡ siÄ™ reprezentacji danych, ktÃ³re sÄ… zbyt wraÅ¼liwe na niewielkie odchylenia w danych wejÅ›ciowych. MoÅ¼e byÄ‡ uÅ¼yty jako technika obrony przed atakami unikowymi, poniewaÅ¼ dwie podobne do siebie prÃ³bki danych wejÅ›ciowych powinny w tym przypadku uzyskaÄ‡ rÃ³wnieÅ¼ podobne do siebie kodowania.

### Wariacyjne (ang. variational autoencoders VAE)
[2] SÄ… autoenkoderami generatywnymi. Na podstawie danych ze zbioru uczÄ…cego potrafiÄ… generowaÄ‡ losowe nowe wyniki (podobnie jak przy GAN).
RÃ³Å¼nica architektoniczna tego typu autoenkodera leÅ¼y w budowie kodera. Wynikiem kodera sÄ… tutaj dwa wektory o jednakowych rozmiarach.
Pierwszy z nich jest wektorem Å›rednich, a drugi wektorem odchyleÅ„ standardowych. NastÄ™pnie na ich podstawie liczony jest zakodowany wektor reprezentacji danych, ktÃ³ry zostanie odkodowany wedÅ‚ug standardowej procedury.
DziÄ™ki takiemu podejÅ›ciu autoenkoder staÅ‚ siÄ™ tak naprawdÄ™ generatorem nowych prÃ³bek, ktÃ³re sÄ… z zakresu rozkÅ‚adu danych treningowych.

![Architektura autoenkodera wariacyjnego [2]](../resources/variational-autoencoder.png)  
*Architektura autoenkodera wariacyjnego [2]*

### Antagonistyczne (ang adversarial autoencoders)
[3] SÄ… poÅ‚Ä…czeniem koncepcji autoenkoderÃ³w wariacyjnych z generatywnymi sieciami antagonistycznymi. Tak naprawdÄ™ sÄ… to rÃ³Å¼ne podejÅ›cia do problemu generowania danych.
Wektor zakodowany podobnie jak w przypadku VAE skÅ‚ada siÄ™ z dwÃ³ch podwektorÃ³w â€“ Å›redniej wartoÅ›ci i odchylenia standardowego. W tym przypadku staramy siÄ™ wymusiÄ‡ na koderze, by podÄ…Å¼aÅ‚ znanym rozkÅ‚adem.
Architektura tego rozwiÄ…zania bazuje na architekturze antagonistycznych sieci generatywnych. Do dyskryminatora, tak jak i do dekodera dostarczamy zakodowany wektor reprezentacji danych wejÅ›ciowych.
Do dyskryminatora dostarczany jest rÃ³wnieÅ¼ rozkÅ‚ad danych treningowych.

![Architektura autoenkodera antagonistycznego [2]](../resources/adversarial-autoencoder.png)  
*Architektura autoenkodera antagonistycznego [2]*

Matematyczny zapis architektury antagonistycznego autoenkodera:

```
ğ‘¥ âˆ’ ğ‘‘ğ‘ğ‘›ğ‘’ ğ‘¤ğ‘’ğ‘—Å›ğ‘ğ‘–ğ‘œğ‘¤ğ‘’
ğ‘§ âˆ’ ğ‘¤ğ‘¦ğ‘›ğ‘–ğ‘˜ ğ‘˜ğ‘œğ‘‘ğ‘’ğ‘Ÿğ‘
ğ‘(ğ‘§|ğ‘¥) âˆ’ ğ‘Ÿğ‘œğ‘§ğ‘˜Å‚ğ‘ğ‘‘ ğ‘˜ğ‘œğ‘‘ğ‘œğ‘¤ğ‘ğ‘›ğ‘–ğ‘
ğ‘(ğ‘¥|ğ‘§) âˆ’ ğ‘Ÿğ‘œğ‘§ğ‘˜Å‚ğ‘ğ‘‘ ğ‘‘ğ‘’ğ‘˜ğ‘œğ‘‘ğ‘œğ‘¤ğ‘ğ‘›ğ‘–ğ‘
ğ‘ğ‘‘(ğ‘¥) âˆ’ ğ‘Ÿğ‘œğ‘§ğ‘˜Å‚ğ‘ğ‘‘ ğ‘‘ğ‘ğ‘›ğ‘¦ğ‘h
ğ‘(ğ‘¥) âˆ’ ğ‘Ÿğ‘œğ‘§ğ‘˜Å‚ğ‘ğ‘‘ ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ğ‘¢
ğ‘(ğ‘§) âˆ’ ğ‘Ÿğ‘œğ‘§ğ‘˜Å‚ğ‘ğ‘‘ ğ‘—ğ‘ğ‘˜ğ‘– ğ‘hğ‘ğ‘’ğ‘šğ‘¦ ğ‘›ğ‘Å‚ğ‘œÅ¼ğ‘¦Ä‡ ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘˜ğ‘¡ğ‘œğ‘Ÿ ğ‘§
ğ‘(ğ‘§) = âˆ« ğ‘(ğ‘§|ğ‘¥)ğ‘ğ‘‘(ğ‘¥)ğ‘‘ğ‘¥
```

![Realistyczne twarze wygenerowane przy pomocy antagonistycznych autoenkoderÃ³w [4]](../resources/adversarial-autoencoder-faces.png)  
*Realistyczne twarze wygenerowane przy pomocy antagonistycznych autoenkoderÃ³w[4]*

## Å¹rÃ³dÅ‚a
* [[1] M. Mamczur, â€Czym sÄ… autoenkodery (autokodery) i jakie maja zastosowanie?,â€](www.miroslawmamczur.pl/czym-sa-autoenkodery-autokodery-i-jakie-maja-zastosowanie/)
* [[2] IntroductiontoAdversarialAutoencoders](https://rubikscode.net/2019/01/14/introduction-to-adversarial-autoencoders/)
* [[3] S. Madeline i R. Harang, â€Adversarial Autoencoders](https://www.sophos.com/en-us/medialibrary/PDFs/technical-papers/Adversarial-Autoencoders.pdf)
* [[4] Adversarial Autoencoder Achieves GAN-level Results](https://neurohive.io/en/news/adversarial-autoencoder-achieves-gan-level-results/)
